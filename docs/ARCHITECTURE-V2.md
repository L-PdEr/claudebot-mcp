# ClaudeBot v2 - Optimierte Architektur

> Basierend auf Industry Benchmarks: Mem0, Letta/MemGPT, Zep, MIRIX

---

## üèÜ Benchmark-Vergleich

| System | J-Score | St√§rke | ClaudeBot Status |
|--------|---------|--------|------------------|
| **MIRIX** | 78%+ | Multi-Agent | Ziel |
| **Zep** | 76.6% | Knowledge Graph | ‚úÖ Graph implementiert |
| **Letta/MemGPT** | ~74% | Document Analysis | ‚ö†Ô∏è Kein Wake/Sleep |
| **Mem0g** | 68.5% | Temporal Queries | ‚úÖ Timestamps vorhanden |
| **Mem0** | 66.9% | Production Ready | ‚úÖ Baseline erreicht |

---

## üéØ Was wir HABEN ‚úÖ

### 1. Prompt Caching (90% Ersparnis)
```rust
// src/claude.rs - IMPLEMENTIERT
.header("anthropic-beta", "prompt-caching-2024-07-31")
cache_control: { type: "ephemeral" }
```

### 2. Response Caching (~20% Ersparnis)
```rust
// src/cache.rs - IMPLEMENTIERT
pub struct ResponseCache {
    cache: moka::future::Cache<String, CachedResponse>,
}
```

### 3. Graph Memory (Relational)
```rust
// src/graph.rs - IMPLEMENTIERT
pub enum EntityType { Project, Person, Technology, Preference, ... }
pub enum RelationType { WorksOn, Prefers, Knows, Uses, ... }
```

### 4. Model Routing (3-Tier)
```rust
// src/router.rs - IMPLEMENTIERT
pub enum ModelHint {
    Haiku,   // $0.25/M - 80% queries
    Sonnet,  // $3/M    - 15% queries
    Opus,    // $15/M   - 5% queries
}
```

### 5. Llama Classification
```rust
// src/router.rs:193-261 - IMPLEMENTIERT
async fn route_with_llama(query: &str) -> RouteResult
// Ollama llama3.2:3b f√ºr Komplexit√§ts-Klassifikation
```

---

## üî• Was FEHLT ‚ùå

### 1. Llama Compression Worker (KRITISCH)

**Problem:** Context w√§chst unbegrenzt ‚Üí Token-Kosten explodieren

**L√∂sung:**
```rust
// src/llama_worker.rs - NEU ERSTELLEN
pub struct LlamaWorker {
    ollama_url: String,
    model: String,  // llama3.2:3b
}

impl LlamaWorker {
    /// Komprimiert lange Konversationen auf Kernaussagen
    pub async fn compress_context(&self, messages: &[Message], target_tokens: usize) -> Result<String> {
        let prompt = format!(
            "Komprimiere diese Konversation auf die wichtigsten Fakten und Entscheidungen. \
            Ziel: max {} Tokens. Behalte: Namen, Zahlen, Entscheidungen, TODOs.\n\n{}",
            target_tokens,
            messages.iter().map(|m| m.content.as_str()).collect::<Vec<_>>().join("\n---\n")
        );

        self.generate(&prompt).await
    }

    /// Extrahiert Entit√§ten und Relationen f√ºr Graph Memory
    pub async fn extract_entities(&self, text: &str) -> Result<Vec<Entity>> {
        let prompt = format!(
            "Extrahiere Entit√§ten aus diesem Text als JSON:\n\
            Format: {{\"entities\": [{{\"name\": \"...\", \"type\": \"person|project|tech\", \"context\": \"...\"}}]}}\n\n{}",
            text
        );

        let response = self.generate(&prompt).await?;
        serde_json::from_str(&response)
    }

    /// Klassifiziert Query-Komplexit√§t f√ºr Model-Routing
    pub async fn classify_complexity(&self, query: &str) -> QueryComplexity {
        // Bereits implementiert in router.rs, hierher verschieben
    }
}
```

### 2. Wake/Sleep Cycle (MemGPT-Style)

**Problem:** Bot ist immer "wach" ‚Üí keine Hintergrund-Verarbeitung

**L√∂sung:**
```rust
// src/lifecycle.rs - NEU ERSTELLEN
pub struct WakeSleepCycle {
    state: Arc<AtomicU8>,  // 0=Sleep, 1=Wake, 2=Processing
    idle_timeout: Duration,
    last_activity: Arc<AtomicI64>,
}

impl WakeSleepCycle {
    pub async fn run(&self) {
        loop {
            match self.current_state() {
                State::Sleep => {
                    // Hintergrund-Tasks
                    self.consolidate_memories().await;
                    self.decay_old_memories().await;
                    self.compress_long_conversations().await;

                    tokio::time::sleep(Duration::from_secs(60)).await;
                }
                State::Wake => {
                    // Warte auf Aktivit√§t oder Timeout
                    if self.idle_for() > self.idle_timeout {
                        self.transition_to_sleep();
                    }
                    tokio::time::sleep(Duration::from_secs(1)).await;
                }
                State::Processing => {
                    // Aktive Verarbeitung, nicht st√∂ren
                    tokio::time::sleep(Duration::from_millis(100)).await;
                }
            }
        }
    }

    async fn consolidate_memories(&self) {
        // √Ñhnliche Memories zusammenf√ºhren
        // Graph-Relationen st√§rken
        // Ebbinghaus-Decay anwenden
    }
}
```

### 3. Vector Embeddings (Semantic Search)

**Problem:** FTS5 ist keyword-basiert, nicht semantisch

**L√∂sung:**
```rust
// src/embeddings.rs - NEU ERSTELLEN
pub struct EmbeddingStore {
    model: String,  // "nomic-embed-text" via Ollama
    dimension: usize,  // 384 oder 768
}

impl EmbeddingStore {
    /// Generiert Embedding lokal via Ollama
    pub async fn embed(&self, text: &str) -> Result<Vec<f32>> {
        let response = reqwest::Client::new()
            .post(&format!("{}/api/embeddings", self.ollama_url))
            .json(&json!({
                "model": self.model,
                "prompt": text
            }))
            .send()
            .await?;

        let result: EmbeddingResponse = response.json().await?;
        Ok(result.embedding)
    }

    /// Cosine Similarity f√ºr Semantic Search
    pub fn similarity(a: &[f32], b: &[f32]) -> f32 {
        let dot: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();
        let norm_a: f32 = a.iter().map(|x| x * x).sum::<f32>().sqrt();
        let norm_b: f32 = b.iter().map(|x| x * x).sum::<f32>().sqrt();
        dot / (norm_a * norm_b)
    }
}

// SQLite Schema-Erweiterung f√ºr Embeddings
// (Alternative zu pgvector - funktioniert mit sqlite-vss)
```

### 4. Token Pre-counting

**Problem:** Kosten erst nach API-Call bekannt

**L√∂sung:**
```rust
// src/tokenizer.rs - NEU ERSTELLEN
use tiktoken_rs::cl100k_base;

pub struct TokenCounter {
    bpe: tiktoken_rs::CoreBPE,
}

impl TokenCounter {
    pub fn count(&self, text: &str) -> usize {
        self.bpe.encode_with_special_tokens(text).len()
    }

    pub fn estimate_cost(&self, input: &str, output_estimate: usize, model: &ModelHint) -> f64 {
        let input_tokens = self.count(input);
        let (input_price, output_price) = match model {
            ModelHint::Haiku => (0.25 / 1_000_000.0, 1.25 / 1_000_000.0),
            ModelHint::Sonnet => (3.0 / 1_000_000.0, 15.0 / 1_000_000.0),
            ModelHint::Opus => (15.0 / 1_000_000.0, 75.0 / 1_000_000.0),
        };

        (input_tokens as f64 * input_price) + (output_estimate as f64 * output_price)
    }

    /// Warnt wenn Budget √ºberschritten wird
    pub fn check_budget(&self, input: &str, daily_remaining: f64, model: &ModelHint) -> BudgetCheck {
        let estimated = self.estimate_cost(input, 2000, model);
        if estimated > daily_remaining {
            BudgetCheck::Exceeded { estimated, remaining: daily_remaining }
        } else {
            BudgetCheck::Ok { estimated }
        }
    }
}
```

---

## üìê Optimierte Architektur v2

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      ClaudeBot v2 Pipeline                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  User Message                                                       ‚îÇ
‚îÇ       ‚îÇ                                                             ‚îÇ
‚îÇ       ‚ñº                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                               ‚îÇ
‚îÇ  ‚îÇ Token Count ‚îÇ ‚îÄ‚îÄ‚ñ∫ ‚îÇBudget Check ‚îÇ ‚îÄ‚îÄ‚ñ∫ Exceeded? ‚Üí Warn User     ‚îÇ
‚îÇ  ‚îÇ (tiktoken)  ‚îÇ     ‚îÇ Pre-flight  ‚îÇ                               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                               ‚îÇ
‚îÇ         ‚îÇ                    ‚îÇ                                      ‚îÇ
‚îÇ         ‚ñº                    ‚ñº                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                               ‚îÇ
‚îÇ  ‚îÇ       Response Cache            ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ   (SHA256 Context-Aware)        ‚îÇ ‚îÄ‚îÄ‚ñ∫ Hit? ‚Üí Return (0 cost)    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                               ‚îÇ
‚îÇ                 ‚îÇ Miss                                              ‚îÇ
‚îÇ                 ‚ñº                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                               ‚îÇ
‚îÇ  ‚îÇ     Llama Classifier            ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ   (Query Complexity)            ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ   Simple ‚Üí Haiku (80%)          ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ   Moderate ‚Üí Sonnet (15%)       ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ   Complex ‚Üí Opus (5%)           ‚îÇ                               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                               ‚îÇ
‚îÇ                 ‚îÇ                                                   ‚îÇ
‚îÇ                 ‚ñº                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                               ‚îÇ
‚îÇ  ‚îÇ      Memory Retrieval           ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Vector  ‚îÇ + ‚îÇ   Graph     ‚îÇ  ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ(Ollama) ‚îÇ   ‚îÇ(Relational) ‚îÇ  ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ               ‚ñº                 ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ        Hybrid Score             ‚îÇ                               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                               ‚îÇ
‚îÇ                 ‚îÇ                                                   ‚îÇ
‚îÇ                 ‚ñº                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                               ‚îÇ
‚îÇ  ‚îÇ     Context Compression         ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ   (Llama wenn > 4K tokens)      ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ Wake/Sleep pr√ºft        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       periodisch              ‚îÇ
‚îÇ                 ‚îÇ                                                   ‚îÇ
‚îÇ                 ‚ñº                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                               ‚îÇ
‚îÇ  ‚îÇ       Claude API Call           ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ   WITH PROMPT CACHING           ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ   (Static: -90% cost)           ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ   (Session: -90% cost)          ‚îÇ                               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                               ‚îÇ
‚îÇ                 ‚îÇ                                                   ‚îÇ
‚îÇ                 ‚ñº                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                               ‚îÇ
‚îÇ  ‚îÇ     Post-Processing             ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ  - Entity Extraction (Llama)    ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ  - Graph Update                 ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ  - Memory Store                 ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ  - Response Cache Update        ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ  - Usage Tracking               ‚îÇ                               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                               ‚îÇ
‚îÇ                 ‚îÇ                                                   ‚îÇ
‚îÇ                 ‚ñº                                                   ‚îÇ
‚îÇ           Response to User                                          ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê   ‚îÇ
‚îÇ                    Background Tasks                                 ‚îÇ
‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê   ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                               ‚îÇ
‚îÇ  ‚îÇ       Wake/Sleep Cycle          ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ  - Memory Consolidation         ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ  - Ebbinghaus Decay             ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ  - Context Compression          ‚îÇ                               ‚îÇ
‚îÇ  ‚îÇ  - Graph Pruning                ‚îÇ                               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                               ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üí∞ Kostenoptimierung Zusammenfassung

| Optimierung | Implementiert | Ersparnis | Priorit√§t |
|-------------|---------------|-----------|-----------|
| Prompt Caching | ‚úÖ | -90% Input | - |
| Response Caching | ‚úÖ | -20% Overall | - |
| Model Routing | ‚úÖ | -53% (vs nur Sonnet) | - |
| **Llama Compression** | ‚ùå | -30-40% Tokens | HOCH |
| **Token Pre-count** | ‚ùå | Budget Control | HOCH |
| **Vector Embeddings** | ‚ùå | +10% Accuracy | MITTEL |
| **Wake/Sleep** | ‚ùå | Background Opt | MITTEL |
| Batch Processing | ‚ùå | -40% Latenz | NIEDRIG |

---

## üéØ N√§chste Schritte

### Phase 1: Token Control (1-2 Tage)
1. `src/tokenizer.rs` - Token counting mit tiktoken
2. Pre-flight Budget-Check
3. Warnung bei Budget-√úberschreitung

### Phase 2: Llama Worker (2-3 Tage)
1. `src/llama_worker.rs` - Compression + Entity Extraction
2. Router-Integration refactoren
3. Context-Management bei > 4K tokens

### Phase 3: Embeddings (2-3 Tage)
1. `src/embeddings.rs` - Ollama nomic-embed-text
2. SQLite VSS Extension oder eigene Cosine-Similarity
3. Hybrid Retrieval (FTS5 + Vector)

### Phase 4: Wake/Sleep (1-2 Tage)
1. `src/lifecycle.rs` - State Machine
2. Background Tasks (Consolidation, Decay)
3. Idle Detection

---

## üìö Referenzen

- [Mem0 Benchmark](https://arxiv.org/abs/2504.19413) - LOCOMO J-Score Methodik
- [Letta Memory](https://www.letta.com/blog/benchmarking-ai-agent-memory) - MemGPT Architektur
- [Anthropic Prompt Caching](https://www.anthropic.com/news/prompt-caching) - 90% Kostenreduktion
- [Token-Efficient Data Prep](https://thenewstack.io/a-guide-to-token-efficient-data-prep-for-llm-workloads/) - 30-40% Einsparung
- [Cost-Effective LLM Apps](https://www.glukhov.org/post/2025/11/cost-effective-llm-applications/) - Model Routing

---

*Erstellt: 2026-01-28*
*Status: Architektur-Review*
